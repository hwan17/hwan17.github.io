---
title: MRC(trainer)
excerpt: 학습되는 동안 trainer 관련 문서 정리

categories:
  - project
tags:
  - [Library, Project]

toc: true
toc_sticky: true

date: 2021-05-03
last_modified_at: 2021-05-03
---

# Library [transformers]

### Trainer, TrainingArguments  

## Trainer
- get_train_dataloader
- get_eval_dataloader
- get_test_dataloader
- log
- create_optimizer_and_scheduler
- create_optimizer
- create_scheduler
- compute_loss
- training_step
- prediction_step
- evaluate(eval_dataset, ignore_keys, metric_key_prefix)
- predict

.........
- model
- args
- data_collator
- train_dataset
- eval_dataset
- tokenizer
- model_init
- compute_metrics
- callbacks
- optimizers - A tuple containing the optimizer and the scheduler to use ( default : AdamW, get_linear_scheduler_with_warmup())

...........

TrainArguments

TrainingArguments(
    
    output_dir=outputs,  # 출력 디렉토리
    overwrite_output_dir=False,  # 덮어쓰기 옵션
    do_train=True,  # 학습 옵션
    do_eval=True,   # 검증 옵션 
    do_predict=False, #  평가 옵션
    evaluation_strategy=IntervalStrategy.NO,  # 검증 단위
    prediction_loss_only=False,    # 검증 및 평가 수행시 로스만 리턴(?)
    per_device_train_batch_size=16, #배치사이즈
    per_device_eval_batch_size=16,  # 검증 배치사이즈
    gradient_accumulation_steps=1,  # 기울기 축적 스텝
    eval_accumulation_steps=None,  # 
    learning_rate=3e-05,  # 학습률
    weight_decay=0.01,  # 
    adam_beta1=0.9, 
    adam_beta2=0.999, 
    adam_epsilon=1e-08, 
    max_grad_norm=1.0, # 최대 기울기 norm
    num_train_epochs=5,  # 에폭
    max_steps=-1,  # 트레인 단계 수(?)
    lr_scheduler_type=SchedulerType.LINEAR, # 학습률 스케줄러
    warmup_ratio=0.0, # 0부터 토탈 트레이닝 스텝에 사용하는 비율 리니어 웜업
    warmup_steps=0,  # 
    logging_dir=runs/Apr29_05-41-20_c39e8a240d24, 
    logging_strategy=IntervalStrategy.STEPS, # 로깅 단위 종류
    logging_first_step=False, 
    logging_steps=500, # 로깅 단위
    save_strategy=IntervalStrategy.NO, # 저장 단위
    save_steps=500, 
    save_total_limit=None, # 최대 저장 개수
    no_cuda=False, # 쿠다 미사용
    seed=42, 
    fp16=False, # fp16 적용 여부
    fp16_opt_level=O1,  # fp16 종류
    fp16_backend=auto,  # 버전에 맞게 써야
    fp16_full_eval=False, # 메모리적게 사용할 수 있도록 평가에도 적용
    local_rank=-1, 
    tpu_num_cores=None, 
    tpu_metrics_debug=False, 
    debug=False, 
    dataloader_drop_last=False, # 배치 마지막 버림
    eval_steps=500,  # eval_strategy가 스텝일 때 단위 로그 스텝도 동일하게 세팅
    dataloader_num_workers=0, 
    past_index=-1,# XL관련 모델
    run_name=outputs, # wandb쓸때 런네임
    disable_tqdm=False, 
    remove_unused_columns=True, 
    label_names=None, 
    load_best_model_at_end=False, 
    metric_for_best_model=None, 
    greater_is_better=None, 
    ignore_data_skip=False, 
    sharded_ddp=[], 
    deepspeed=None, 
    label_smoothing_factor=0.0, 
    adafactor=False, # adamw 대신 다른 옵티마이저 사용
    group_by_length=False, 
    report_to=[], 
    ddp_find_unused_parameters=None, 
    dataloader_pin_memory=True, 
    skip_memory_metrics=False, 
    _n_gpu=1
    )